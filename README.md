# Variational_Auto_Encoder
A Variational Autoencoder (VAE) is a type of generative model that is particularly useful for unsupervised learning tasks. It is a kind of autoencoder that learns a latent space representation of the input data and is able to generate new data samples from this latent space. The key idea behind VAEs is to combine principles from neural networks and Bayesian inference.

Key Concepts of VAEs
Autoencoder Basics:

Encoder: Maps the input data 
ğ‘¥
x to a latent representation 
ğ‘§
z. In a traditional autoencoder, this mapping is deterministic.
Decoder: Maps the latent representation 
ğ‘§
z back to the data space to reconstruct the input 
ğ‘¥
^
x
^
 .
The objective is to minimize the reconstruction loss between the input 
ğ‘¥
x and the reconstructed output 
ğ‘¥
^
x
^
 .
Variational Aspect:

Instead of mapping the input to a single point in the latent space, the VAE maps the input to a distribution over the latent space. Typically, a Gaussian distribution is used.
Encoder Output: Instead of outputting 
ğ‘§
z, the encoder outputs parameters of the distribution (mean 
ğœ‡
Î¼ and standard deviation 
ğœ
Ïƒ).
Sampling:

To generate a sample from the latent space during training, we sample 
ğ‘§
z from the distribution parameterized by 
ğœ‡
Î¼ and 
ğœ
Ïƒ. This sampling step introduces stochasticity.
Reparameterization Trick: To backpropagate through the stochastic sampling process, the reparameterization trick is used. This involves sampling 
ğœ–
Ïµ from a standard normal distribution and computing 
ğ‘§
z as:
ğ‘§
=
ğœ‡
+
ğœ
â‹…
ğœ–
z=Î¼+Ïƒâ‹…Ïµ
Loss Function:

The VAE loss function has two components:
Reconstruction Loss: Measures how well the decoder reconstructs the input from the sampled latent variable 
ğ‘§
z. Typically, this is the mean squared error or binary cross-entropy.
KL Divergence Loss: Measures how closely the learned latent distribution matches the prior distribution (usually a standard normal distribution). This regularizes the latent space to ensure that it follows the desired distribution.
VAEÂ Loss
=
ReconstructionÂ Loss
+
KLÂ DivergenceÂ Loss
VAEÂ Loss=ReconstructionÂ Loss+KLÂ DivergenceÂ Loss
Mathematical Formulation
Given an input 
ğ‘¥
x, the encoder outputs 
ğœ‡
(
ğ‘¥
)
Î¼(x) and 
ğœ
(
ğ‘¥
)
Ïƒ(x). A latent variable 
ğ‘§
z is sampled from the distribution 
ğ‘
(
ğœ‡
(
ğ‘¥
)
,
ğœ
(
ğ‘¥
)
2
)
N(Î¼(x),Ïƒ(x) 
2
 ). The decoder then reconstructs 
ğ‘¥
x from 
ğ‘§
z.

Encoder: 
ğ‘
ğœ™
(
ğ‘§
âˆ£
ğ‘¥
)
q 
Ï•
â€‹
 (zâˆ£x) outputs 
ğœ‡
(
ğ‘¥
)
Î¼(x) and 
ğœ
(
ğ‘¥
)
Ïƒ(x).
Decoder: 
ğ‘
ğœƒ
(
ğ‘¥
âˆ£
ğ‘§
)
p 
Î¸
â€‹
 (xâˆ£z) reconstructs 
ğ‘¥
x from 
ğ‘§
z.
Loss Function:
ğ¿
=
ğ¸
ğ‘
ğœ™
(
ğ‘§
âˆ£
ğ‘¥
)
[
log
â¡
ğ‘
ğœƒ
(
ğ‘¥
âˆ£
ğ‘§
)
]
âˆ’
KL
(
ğ‘
ğœ™
(
ğ‘§
âˆ£
ğ‘¥
)
âˆ¥
ğ‘
(
ğ‘§
)
)
L=E 
q 
Ï•
â€‹
 (zâˆ£x)
â€‹
 [logp 
Î¸
â€‹
 (xâˆ£z)]âˆ’KL(q 
Ï•
â€‹
 (zâˆ£x)âˆ¥p(z))


![Screenshot 2024-03-21 142353](https://github.com/UditKushwaha17/Variational_Auto_Encoder/assets/144841149/4c99a421-83cb-4326-b7a8-54f7e4398da1)
